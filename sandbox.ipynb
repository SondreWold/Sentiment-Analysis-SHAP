{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AdamW\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NorecDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        df = pd.read_csv(path, sep=',')\n",
    "        df.columns = [\"Sentiment\", \"Document\"]\n",
    "        self.documents = list(df['Document'])\n",
    "        self.labels = list(df['Sentiment'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.documents[idx], torch.LongTensor([self.labels[idx]]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = NorecDataset(\"./data/train.csv\")\n",
    "val_set = NorecDataset(\"./data/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(folder: str, name: str, epoch: int, model, train_time_sum, parameters) -> None:\n",
    "\n",
    "    try:\n",
    "        os.makedirs(folder)\n",
    "    except OSError as e:\n",
    "        if(epoch == 0):\n",
    "            print(\"Folder already exist!\")\n",
    "\n",
    "    path = folder + name + \".pt\"\n",
    "\n",
    "    torch.save({\n",
    "        \"name\": name,\n",
    "        \"epoch\": epoch,\n",
    "        \"train_time_sum\": train_time_sum,\n",
    "        \"parameters\": parameters\n",
    "    }, path)\n",
    "\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_to_save.save_pretrained(folder)\n",
    "\n",
    "    print(f\"Saved model to {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_set, val_set):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"NbAiLab/nb-bert-base\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"NbAiLab/nb-bert-base\", num_labels=2)\n",
    "    train_loader = DataLoader(\n",
    "        train_set, batch_size=1, shuffle=True)\n",
    "    val_loader = DataLoader(\n",
    "        val_set, batch_size=1, shuffle=True)\n",
    "    epochs = 2\n",
    "    optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "        \n",
    "    folder = \"./models/\"\n",
    "    name = \"test\"\n",
    "\n",
    "    best_val_loss = 999999\n",
    "    train_time_sum = 0\n",
    "    model = model.to(dev)\n",
    "    checkpointing_enabled = True\n",
    "    for epoch in range(0, epochs):\n",
    "        print(\"\\n\")\n",
    "        print(f\"----- Starting epoch: {epoch} -----\")\n",
    "        start_epoch_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        total_train_acc = 0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "        training_passes = 0\n",
    "        validation_passes = 0\n",
    "        \n",
    "        model.train()\n",
    "        for i, batch in enumerate(tqdm.tqdm(train_loader)):\n",
    "                optimizer.zero_grad()\n",
    "                doc, y = batch\n",
    "                doc = list(doc)\n",
    "                X = tokenizer(doc, add_special_tokens=True, max_length=256, padding=True, return_attention_mask=True,\n",
    "                              return_tensors=\"pt\")\n",
    "                input_ids = X[\"input_ids\"]\n",
    "                token_type_ids = X[\"token_type_ids\"]\n",
    "                attention_mask = X[\"attention_mask\"]\n",
    "\n",
    "                loss, prediction = model(input_ids=input_ids, token_type_ids=token_type_ids,\n",
    "                                         attention_mask=attention_mask, labels=y.squeeze()).values()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()        \n",
    "                training_passes += 1\n",
    "                \n",
    "        \n",
    "        for i, batch in enumerate(tqdm.tqdm(val_loader)):\n",
    "                optimizer.zero_grad()\n",
    "                doc, y = batch\n",
    "                doc = list(doc)\n",
    "                y = y.to(dev)\n",
    "                X = tokenizer(doc, add_special_tokens=True, max_length=256, padding=True, return_attention_mask=True,\n",
    "                              return_tensors=\"pt\")\n",
    "                input_ids = X[\"input_ids\"].to(dev)\n",
    "                token_type_ids = X[\"token_type_ids\"].to(dev)\n",
    "                attention_mask = X[\"attention_mask\"].to(dev)\n",
    "\n",
    "                loss, prediction = model(input_ids=input_ids, token_type_ids=token_type_ids,\n",
    "                                         attention_mask=attention_mask, labels=y.squeeze()).values()\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()        \n",
    "                validation_passes += 1\n",
    "\n",
    "        \n",
    "        end_epoch_time = time.time()\n",
    "        epoch_time = end_epoch_time - start_epoch_time\n",
    "        train_loss = total_train_loss/training_passes\n",
    "        val_loss = total_val_loss/validation_passes\n",
    "        \n",
    "        print(f\"Epoch: {epoch}. Training loss: {train_loss}, val loss: {validation_loss}, epoch time: {epoch_time}\")\n",
    "        \n",
    "        \n",
    "        if val_loss < best_val_loss and checkpointing_enabled:\n",
    "            parameters = sum(\n",
    "                p.numel() for p in model.parameters() if p.requires_grad)\n",
    "            print(\n",
    "                f\"Val loss is lower than previous best which was at {best_val_loss}. Saving model, with number of parameters: {parameters}\")\n",
    "            save_model(folder, name, epoch, model, train_time_sum, parameters)\n",
    "            best_val_loss = val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at NbAiLab/nb-bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/2674 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----- Starting epoch: 0 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2674 [00:11<1:22:54,  1.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f23a815351aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-8a88ad39af4a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_set, val_set)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mtraining_passes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/in4080/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_set,val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
